{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code for scraping URLs, specifically from the Huffington Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "# Beautiful Soup module collects the HTML code from website\n",
    "from bs4 import BeautifulSoup as bs\n",
    "# Selenium allows us to execute cross-browser tests\n",
    "from selenium import webdriver\n",
    "# Webdriver is an intermediatary between the operating system and the device\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this list contains the urls being scraped \n",
    "urls_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opens Chrome browser\n",
    "browser = webdriver.Chrome(ChromeDriverManager().install())\n",
    "browser.maximize_window()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 2000\n",
    "start = 20000\n",
    "\n",
    "for i in range (0,6000):\n",
    "    \n",
    "    # opens the URL containing the environment articles, greater values of 'i' direct to older pages\n",
    "    browser.get(f\"https://www.huffpost.com/impact/green?page={i}\") \n",
    "\n",
    "    # collects the HTML code of the website\n",
    "    get_page = bs(browser.page_source) \n",
    "\n",
    "    # the website has three 'content zones' that contain URLs\n",
    "    for z in range(0,3): \n",
    "        \n",
    "        # the for loop searches for the HTML tags that contain the URLs\n",
    "        for stories in get_page('div',{'class':'zone__content'})[z]('div',{'class':'card card--standard js-card'}):\n",
    "\n",
    "            # searches for the tags and appends the URL to the list\n",
    "            urls_list.append(stories('a')[0]['href']) \n",
    "\n",
    "            # checkpoint saved for every 2000 URLs scraped\n",
    "            if len(urls_list) % checkpoint == 0: \n",
    "\n",
    "                # saves the URLs in a JSON file\n",
    "                with open(f\"huffpost_article_start={start}_end={start + checkpoint}.json\",\"w\") as file:\n",
    "                    json.dump(urls_list, file, ensure_ascii=True)\n",
    "                    file.close()\n",
    "\n",
    "                start += checkpoint\n",
    "                urls_list.clear()\n",
    "\n",
    "                print(f\"--- Checkpoint {start} URLs---\")\n",
    "\n",
    "                time.sleep(2)\n",
    "\n",
    "    print(f\"--- Finished page {i} ---\")\n",
    "\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: each website will require a different URL scraping algorithm, depending on the design and the HTML elements used"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
