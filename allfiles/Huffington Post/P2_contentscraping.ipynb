{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Content scraping \n",
    "\n",
    "This notebook contains the code for scraping the articles' contents, including the title and text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from lxml import etree\n",
    "import xmltodict\n",
    "import urllib3\n",
    "from selenium.common.exceptions import InvalidSessionIdException\n",
    "from selenium.common.exceptions import WebDriverException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list contains all the scraped URLs\n",
    "data = [] \n",
    "\n",
    "# loads all the URLs into the list\n",
    "for i in range(0,38001,2000):\n",
    "    with open(f\"huffpost_article_start={i}_end={i+2000}.json\",\"r\") as file:\n",
    "        data += json.load(file)\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opens Chrome browser\n",
    "browser = webdriver.Chrome(ChromeDriverManager().install())\n",
    "browser.maximize_window()\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list contains all the scraped content\n",
    "huffPost_articles = []\n",
    "\n",
    "# checkpoint saved for every 2000 articles scraped \n",
    "checkpoint = 2000\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for url in data[0:]:\n",
    "\n",
    "    # opens URL in the browser\n",
    "    try:\n",
    "        browser.get(url)\n",
    "    except (InvalidSessionIdException, WebDriverException) as err :\n",
    "        browser = webdriver.Chrome(ChromeDriverManager().install())\n",
    "        browser.maximize_window()\n",
    "        time.sleep(1)\n",
    "        browser.get(url)\n",
    "    \n",
    "    # collects the HTML code of the website \n",
    "    try:\n",
    "        s = bs(browser.page_source)\n",
    "    except WebDriverException:\n",
    "        browser = webdriver.Chrome(ChromeDriverManager().install())\n",
    "        browser.maximize_window()\n",
    "        time.sleep(1)\n",
    "        browser.get(url)\n",
    "        s = bs(browser.page_source)\n",
    "\n",
    "    # each article is stored as a dictionary with 2 keys corresponding to the title and the text\n",
    "    article = {}\n",
    "\n",
    "    # collects title \n",
    "    try:\n",
    "        article['title'] = s('h1',{'class','headline'})[0].get_text().strip()\n",
    "    except IndexError:\n",
    "        continue\n",
    "\n",
    "    # collects text\n",
    "    content = \"\"\n",
    "    for i in s('div',{'class','primary-cli cli cli-text'}):\n",
    "        try:\n",
    "            content += i('p')[0].get_text().strip()\n",
    "        except IndexError:\n",
    "            continue \n",
    "    article['text'] = content\n",
    "\n",
    "    # appends article dictionary to the article list\n",
    "    huffPost_articles.append(article) \n",
    "\n",
    "    time.sleep(4)\n",
    "\n",
    "    # once checkpoint is passed, then the article list is saved as a json file\n",
    "    count += 1\n",
    "    if count % checkpoint == 0:\n",
    "        with open(f\"huffPost_article_{count}.json\",\"w\",encoding=\"utf8\") as file:\n",
    "            json.dump(huffPost_articles, file, ensure_ascii=False)\n",
    "        print(f\"Checkpoint {count} passed-------\")\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checks for number of articles in the article list\n",
    "len(huffPost_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the checkpoint algorithm was not used consistently as the algorithm was not run continuously. The sheer amount of data meant that saving manually using the code below was safer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back up file saving option\n",
    "with open(\"backup.json\",\"w\",encoding=\"utf-8\") as file:\n",
    "    json.dump(huffPost_articles, file, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a42ccb73e7d9bfdf27e036f1d2b8b681e55fc0743cc5586bc2474d4a60f4b886"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
